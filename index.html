<!DOCTYPE html>
<meta charset="UTF-8">
<html lang="fr">
    <head>
        <title>Projet IFT3150</title>
        <link rel="stylesheet" href="style.css">
        <script src="comportement.js"></script>
        <!-- Bootstrap (en commentaire for now mais si on en a besoin)-->
    <!--
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    -->
    </head>

<body>

    <header>
        <div id="header_top">
            <h1>La théorie des fonctions lexicales appliquée pour tester des grands modèles de langages</h1>
            <p>Viviane Binet, Alessandra Mancas</p>
            <p>(20244728), (20249098)</p>
            <p>Dans le cadre du cours IFT3150 - Prof. Philippe Langlais (A24)</p>
        </div>
        <div id="header_nav">
            <h3 class="navhead" id="enonce_projet"><a href="#enonce_projet">Énoncé du projet</a></h3>
            <h3 class="navhead" id="description"><a href="#description">Description détaillée</a></h3>
            <h3 class="navhead" id="plan_avancement"><a href="#plan_avancement">Plan</a></h3>
            <h3 class="navhead" id="rapports_avancement"><a href="#rapports_avancement">Rapports d'avancement</a></h3>
            <h3><a href="todo.html" id="todo-onglet">TODO</a></h3>
        </div>
    </header>
    
    <div id="contents">
    
        <section id="#enonce_projet" class="partie">
                <p>Nous utilisons la ressource du Réseau lexical français (RL-fr) pour en extraire des exemples de fonctions lexicales.
                    Une fonction lexicale retourne un ensemble de mots qui une relation précisée par la fonction avec le mot en entrée.
                    Ces exemples seront de la forme "mot en input" -fonction-> "mot en output". Nous allons ensuite sélectionner certaines
                    fonctions lexicales et les verbaliser pour tester des modèles de langage.  Nous allons donner en entrée aux modèles des
                    phrases du genre "quel est le mot pour désigner le chef d'un navire", qui teste la fonction lexicale <i>chef de()</i>. 
                    Nous allons pouvoir comparer les performances des modèles de langage sur les différentes fonctions.
                    <a href= "https://www.ortolang.fr/market/lexicons/lexical-system-fr/v3.1">Réseau lexical du français RL-fr</a>

                </p>
        </section>
    
        <section id="#description" class="partie hidden">
            <h2>Titre</h2>
            La théorie des fonctions lexicales appliquée pour tester des grands modèles de
            langage: une approche par verbalisation

            <h2>Spécification fonctionnelle</h2>
            Dans ce projet, on s'attend à augmenter un API existant permettant de traiter les données du RL-fr et les mettre dans
            un format propre à notre tâche.
            On doit aussi développer une façon de faire rouler un modèle de langage sur nos ordinateurs, puis d'écrire
            un programme qui peut l'évaluer et nous afficher des résultats de la façon la plus automatique possible.

            <h2>Environnement et contraintes techniques</h2>
            Tout d'abord, nous travaillons sur deux ordinateurs séparés, alors il faut prendre en compte les limitations
            du matériel des deux machines quand ça concerne le roulement du modèle de langage. Nous avons donc gardé
            les dépendances à un minimum.
            Le code du projet est écrit en Python et utilisera simplement les librairies Pandas, Ollama, NumPy et Matplotlib.
            Pour obtenir le modèle et le faire rouler, nous avons utilisé le framework Ollama.
            Établir des conventions de nommage des fichiers pour gérer et traiter plus facilement le contenu avec
            lequel on travaille.

            <h2>Architecture logicielle</h2>
            <ul>
                <li>rl-fr_api.py : Un API, créé en ajoutant nos fonctions au code existant d'Alexander Petrov, pour
                lire les données du RL-fr et construire des échantillons qui fonctionnent pour notre tâche.</li>
                <li>Les exemples pour chaque FL seront dans un fichier .csv</li>
                <li>Un fichier .txt qui contient le "template" de llama3.2 qu'on a écrit</li>
                <li>template_model.py : Un script contenant toutes les fonctions nécessaires pour rouler et évaluer le modèle,
                ainsi que générer les diverses vues qui nous ont servi à l'interprétation des résultats.</li>
                <li>Les sorties du modèle (pour chaque FL, k-shot, # d'échantillon) seront inscrites dans des fichiers .csv.
                On pourra ainsi effectuer des opérations dessus pour faire l'évaluation.</li>
                <li>Les résultats des évaluations seront également affichées dans des .csv, qui est un format simple
                pour générer les tableaux et visualisations à la fin</li>
            </ul>

            <h2>Modules principaux de travail</h2>
            <h3>rl-fr_api.py</h3>
                <ul>
                    <li>Extraire les lexies de chaque nœud du RL-fr</li>
                    <li>Obtenir tous les exemples pour chaque mot-source possible, pour chaque FL</li>
                    <li>Séparer les FLs en paradigmatiques et syntagmatiques</li>
                    <li>Nettoyer les données pour ne pas contenir des chiffres</li>
                    <li>Générer plusieurs échantillons aléatoires pour chaque FL ayant un nombre suffisant
                    d'exemples</li>
                </ul>
            <h3>template_model.py</h3>
                <ul>
                    <li>Contient les listes de questions et d'exemples (à donner pour chaque k-shot), pour chaque
                    FL</li>
                    <li>Pipeline des requêtes: pour une FL donnée, itérer à travers tous les exemples d'un échantillon
                    les noter dans un .csv, calculer le score entre la sortie donnée et celle attendue. Répéter pour chaque
                        question de chaque FL, à chaque k-shot</li>
                    <li>Fournir des exemples au modèle selon le k choisi</li>
                    <li>Compiler les scores de chaque échantillon en faisant leur moyenne, les mettre dans des fichiers .csv</li>
                    <li>Construire les visualisations nécessaires</li>
                </ul>
        </section>
    
        <section id="#plan_avancement" class="partie hidden">
            <table>
                <tr>
                    <td>Date de début</td>
                    <td>6 septembre 2024</td>
                </tr>
                <tr>
                    <td>Date de fin</td>
                    <td>20 décembre 2024</td>
                </tr>
                <tr>
                    <td>Dates prévues pour les modules</td>
                    <td>
                        <ul>
                            <li>API : </li>
                            <li>Développement du modèle: </li>
                            <li>Obtention des résultats</li>
                        </ul>
                    </td>
                </tr>
            </table>
        </section>
    
        <section id="#rapports_avancement" class="partie hidden">
            <h2>Rapports d'avancement</h2>
            <h3>S1 - 02/09 - 08/09</h3>
            <ul>
                <li>Rencontre avec Philippe Langlais le vendredi 6</li>
                <li>Il nous a envoyé des sources préliminaires à lire pour se familiariser avec le RL-fr et la théorie Sens-Texte</li>
            </ul>
            <h3>S2 - 09/09 - 15/09</h3>
            <ul>
                <li>Lecture des documents sur la linguistique (Documentation du RL-fr, Théorie Sens-Texte)</li>
                <li>Rencontre avec Alexander Petrov: présentation du API qu'il a conçu pour manipuler les données du RL-fr</li>
            </ul>
            <h3>S3 - 16/09 - 22/09</h3>
            <ul>
                <li>Rencontre avec M.Langlais pour raffiner le sujet du projet.</li>
                <li>Avant de choisir les FL que nous voudrons verbaliser, il faut dresser tous les exemples tels qu'ils sont
                    décrits dans le texte "Les fonctions lexicales dernier cri" (Mel'čuk, Polguère 2021), c'est à dire sous
                    la forme "source => [séparateur1] exemple1 [séparateur2] exemple2 ...". </li>
                <li>Nous avons donc commencé à travailler sur l'API d'Alexander pour l'adapter à notre tâche, afin d'obtenir un
                    fichier .txt compréhensif contnenant des exemples</li>
            </ul>

            <h3>S4 - 23/09 - 29/09</h3>
            <ul>
                <li>Nous avons obtenu le fichier .txt voulu en modifiant les fonctions write_to_file(), get_endpoints() et
                    get_relation_name() du API qu'on nous a fourni. On peut désormais transformer ces données dans un format
                    qui fonctionne avec notre tâche.
                </li>

            </ul>

            <h3>S5 - 30/09 - 6/10</h3>
                <ul>
                    <li>Nous avons obtenu un fichier .tsv des exemples afin de pouvoir les traiter facilement. Nous avons 
                        produit un fichier texte d'exemples aléatoires pour les fonctions lexicales les plus fréquentes.
                    </li>
                </ul>

            <h3>S6 - 07/10 - 13/10</h3>
                <ul>
                    <li>Grâce aux fichiers d'exemples aléatoires, nous pouvons maintenant entamer les tests avec les modèles
                        de langue en utilisant Ollama.
                    </li>
                    <li>
                        Création d'un script qui, pour une fonction donnée, crée un <i>template</i> pour le modèle, qui sera
                        rempli par chaque mot source qu'on souhaite évaluer. Les résultats du modèle de langue sont ensuite
                        écrits dans un fichier .csv afin qu'on les visualise par rapport aux sorties attendues. Un "success rate"
                        est également calculé pour qu'on mesure rapidement la performance du modèle sur chaque question.
                    </li>
                    <li>
                        Remarque: les réponses à chaque itération du modèle sont parfois légèrement différentes. Évidemment,
                        le RL-FR n'est pas exhaustif dans ses exemples et il se peut que pour une FL, un exemple donné soit valide
                        selon la définition de la FL mais qu'il ne soit pas inclus dans la base de données.
                        Il serait donc intéressant d'écrire des tests ou de quantifier les sorties des modèles pour voir ce qu'ils
                        couvrent et pour comparer les modèles entre eux.
                    </li>
                </ul>
            <h3>S7 - 14/10 - 20/10</h3>
                <ul>
                    <li>Début de la mi-session, alors semaine plus lente. Mais suite des tests sur différentes FLs et questions avec Ollama.</li>
                </ul>
            <h3>S8 - 21/10 - 27/10 </h3>
                <ul>
                    <li>
                        Semaine de relâche + mi-session, donc encore une fois un rythme ralenti.
                    </li>
                    <li>
                        Ollama roule très lentement sur nos deux ordinateurs, soit à cause de problèmes de réseau, soit à cause
                        des limites du matériel (~5 minutes pour 50 exemples)
                    </li>
                    <li>
                        Rencontre avec le professeur pour discuter d'optimisation et de la collecte des résultats. Il propose de
                        se renseigner sur le traitement en <i>batch</i> dans Ollama, pour éviter de lui envoyer une requête plusieurs fois.
                    </li>
                </ul>
            <h3>S9 - 28/10 - 3/11</h3>
                <ul>
                    <li>
                        Recherche sur les méthodes d'optimisation. Il semble que malgré l'option de traiter en batch un modèle avec Ollama,
                        ce dernier nécessite beaucoup de puissance de calcul (CPU, GPU). On essaie de trouver une solution ou une alternative.
                    </li>
                    <li>
                        Alternative possible: ChainForge
                    </li>
                        <ul>
                            <li>
                                Tout se passe en ligne, alors on ne prend pas en compte la machine (rend le travail plus généralisable aussi)
                            </li>
                            <li>
                                Possibilité de tester plusieurs modèles à la fois et comparer leurs sorties. Possible également de tester
                                plusieurs entrées.
                            </li>
                            <li>
                                Interface plutôt simple à utiliser et sortie du modèle affichée de manière intuitive
                            </li>
                            <li>
                                MAIS: plus difficile d'automatiser l'expérience, car pour chaque FL il faut créer des "Prompts nodes" qui
                                comprennent chaque question différente.
                            </li>
                            <li>Certes, il serait intéressant d'utiliser cet outil si le traitement en <i>batch</i> dans Ollama s'avère
                                trop coûteux. Après tout, on ne veut pas endommager nos ordinateurs. </li>
                        </ul>
                    <li>
                        Aussi, ajout d'une fonction qui permet de générer n ensembles aléatoires d'un certain nombre d'exemples
                        pour chaque FL.
                    </li>
                </ul>
                <h3> S10 - 4/11 - 10/11</h3>
                <ul>
                    <li>
                        Nous avons réussi à faire fonctionner de façon efficace le traitement en batch de Ollama. Nous pouvons donc tester
                         nos questions et obtenir des scores pour toutes les fonctions lexicales que nous traitons.
                    </li>
                    <li>
                        Les scores sont pour le moment gardés en mémoire dans des fichiers différents pour chaque fonction lexicale, mais
                        on peut quand même voir facilement quelles questions fonctionnent mieux.
                    </li>
                    <li>
                        Retrait des mots qui contiennent des chiffres ou d'autres caractères non alphabétiques des mots posés au 
                        modèle. 
                    </li>

                </ul>

                <h3> S11 - 11/11 - 17/11</h3>
                <ul>
                    <li>
                        Ajout d'exemples provenant de la ressource aux questions pour voir si on obtient des meilleurs scores. On veut 
                        retirer les mots en exemple des mots demandés au modèle pour ne pas gonfler artificiellement les scores. 
                    </li>
                    <li>
                        Méthode pour mettre toute l'information générée par les tests (les scores) dans un Dataframe, et ensuite pouvoir 
                        extraire ces informations pour présenter des statistiques pour voir quelles questions sont les meilleures, et si 
                        le score augmente avec le nombre d'exemples donnés.
                    </li>
                </ul>

               <h3> S12 - 18/11 - 24/11</h3>
                <ul>
                    <li>
                        Construction de différentes vues pour effectuer par la suite une analyse de nos résultats en utilisant Pandas.
                        <ul>
                            <li>
                                Une vue "sommaire" par FL, contenant les scores moyens pour chaque k-shot, leur variance et le texte
                                de chaque question (au lieu de simplement son index)
                            </li>
                            <li>
                                Par k-shot et parmi toutes les données, des vues avec les meilleures questions et leurs scores, variances
                                et textes respectifs.
                            </li>
                        </ul>
                    </li>
                    <li>
                        Présentation de nos résultats au professeur Langlais. Ok pour compiler le tout dans un rapport.
                    </li>
                    <li>
                        Début de la rédaction du rapport.
                    </li>
                </ul>

                <h3>S13 - 25/11 - 01/12 </h3>
                <ul>
                    <li>
                        Suite de la rédaction du rapport, créations de visualisations à partir de notre code.
                    </li>
                </ul>

                <h3>S14 à 16 - 02/12 - 20/12 </h3>
                <ul>
                    <li>Au cœur de la fin de session, suite et fin de la rédaction du rapport, nettoyage du code source.</li>
                </ul>

        </section>
    
    </div>
</body>
</html>